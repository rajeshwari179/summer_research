{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNItQtsqjt+lzs7OX8Mjgmq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshwari179/summer_research/blob/main/MAPPING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 regularization + spatial loss function"
      ],
      "metadata": {
        "id": "UcrpIpm-NkSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQVcJ2fbNf3l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#grayscale\n",
        "def rgb_to_grayscale(images):\n",
        "    return np.dot(images[..., :3], [0.2989, 0.587, 0.114])\n",
        "\n",
        "x_train_gray = rgb_to_grayscale(x_train)\n",
        "x_test_gray = rgb_to_grayscale(x_test)\n",
        "\n",
        "# Normalize\n",
        "x_train_gray = x_train_gray / 255.0\n",
        "x_test_gray = x_test_gray / 255.0\n",
        "\n",
        "x_train_gray = np.expand_dims(x_train_gray, -1)\n",
        "x_test_gray = np.expand_dims(x_test_gray, -1)\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 1)),\n",
        "    Dense(1024, kernel_regularizer=l1(0.08)),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "    Dense(256, activation=None)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train_gray, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "# Function to manually prune weights\n",
        "def prune_weights(model, threshold):\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, Dense):\n",
        "            weights, biases = layer.get_weights()\n",
        "            new_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
        "            layer.set_weights([new_weights, biases])\n",
        "\n",
        "\n",
        "threshold_value = 0.00008\n",
        "prune_weights(model, threshold_value)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy' , metrics=['accuracy'])\n",
        "\n",
        "# Evaluate the pruned model\n",
        "loss, accuracy = model.evaluate(x_test_gray, y_test)\n",
        "print(f\"Pruned model - Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Function to print model weights\n",
        "def print_model_weights(model):\n",
        "    for layer in model.layers:\n",
        "        weights = layer.get_weights()\n",
        "        if weights:  # Check if there are weights to print\n",
        "            print(f\"Layer: {layer.name}\")\n",
        "            print(\"Weights:\")\n",
        "            print(weights[0])\n",
        "            if len(weights) > 1:  # Check if biases exist\n",
        "                print(\"Biases:\")\n",
        "                print(weights[1])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print_model_weights(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def spatial_loss(outputs):\n",
        "    num_neurons = outputs.size(1)\n",
        "    outputs_np = outputs.detach().cpu().numpy()\n",
        "\n",
        "    # Calculate pairwise correlations\n",
        "    pairwise_correlations = np.zeros((num_neurons, num_neurons))\n",
        "    for i in range(num_neurons):\n",
        "        for j in range(i + 1, num_neurons):\n",
        "            r = np.corrcoef(outputs_np[:, i], outputs_np[:, j])[0, 1]\n",
        "            pairwise_correlations[i, j] = pairwise_correlations[j, i] = r\n",
        "\n",
        "    # Calculate the grid size and ensure it is a perfect square\n",
        "    grid_size = int(np.sqrt(num_neurons))\n",
        "    if grid_size * grid_size != num_neurons:\n",
        "        raise ValueError(\"Number of neurons must be a perfect square to form a grid.\")\n",
        "\n",
        "    # Calculate distances in the grid\n",
        "    distances = np.zeros((num_neurons, num_neurons))\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            for k in range(grid_size):\n",
        "                for l in range(grid_size):\n",
        "                    index_1 = i * grid_size + j\n",
        "                    index_2 = k * grid_size + l\n",
        "                    distance = np.sqrt((i - k)**2 + (j - l)**2)\n",
        "                    distances[index_1, index_2] = 1 / (distance + 1)\n",
        "\n",
        "    # Compute the spatial loss\n",
        "    SL = np.mean(np.abs(pairwise_correlations - distances))\n",
        "\n",
        "    return SL\n"
      ],
      "metadata": {
        "id": "XcurgTxCmNut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal model with l1 regularization, with 50 epochs\n"
      ],
      "metadata": {
        "id": "lAEpEji4YZG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the fully connected network with batch normalization and dropout\n",
        "class FullyConnectedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32)  # Flatten the input\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = FullyConnectedNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to apply L1 regularization\n",
        "def l1_regularization(model, lambda_l1):\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "    return lambda_l1 * l1_norm\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 50\n",
        "lambda_l1 = 0.0001\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss += l1_regularization(model, lambda_l1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ETQ34pFM-w",
        "outputId": "88977269-a8a0-4b32-f859-b2e96707bb59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/50, Loss: 3.7751694045713187\n",
            "Epoch 2/50, Loss: 3.1664483251474094\n",
            "Epoch 3/50, Loss: 3.0900138491559823\n",
            "Epoch 4/50, Loss: 3.0332802595080013\n",
            "Epoch 5/50, Loss: 2.9485762351004365\n",
            "Epoch 6/50, Loss: 2.833741909707599\n",
            "Epoch 7/50, Loss: 2.7150920416083175\n",
            "Epoch 8/50, Loss: 2.6299577472764817\n",
            "Epoch 9/50, Loss: 2.5697404929744008\n",
            "Epoch 10/50, Loss: 2.5226876107628082\n",
            "Epoch 11/50, Loss: 2.4918732143119167\n",
            "Epoch 12/50, Loss: 2.4637931627995524\n",
            "Epoch 13/50, Loss: 2.4444908976859754\n",
            "Epoch 14/50, Loss: 2.4264493641036244\n",
            "Epoch 15/50, Loss: 2.41758079449539\n",
            "Epoch 16/50, Loss: 2.3990146150369474\n",
            "Epoch 17/50, Loss: 2.3904292126140936\n",
            "Epoch 18/50, Loss: 2.376588818362302\n",
            "Epoch 19/50, Loss: 2.3724924359480135\n",
            "Epoch 20/50, Loss: 2.3646449237833242\n",
            "Epoch 21/50, Loss: 2.354778215403447\n",
            "Epoch 22/50, Loss: 2.351242602633698\n",
            "Epoch 23/50, Loss: 2.3418199723333957\n",
            "Epoch 24/50, Loss: 2.3367109140166846\n",
            "Epoch 25/50, Loss: 2.3379541565390194\n",
            "Epoch 26/50, Loss: 2.329679929082046\n",
            "Epoch 27/50, Loss: 2.324726275165977\n",
            "Epoch 28/50, Loss: 2.3222511771999663\n",
            "Epoch 29/50, Loss: 2.3198983090003127\n",
            "Epoch 30/50, Loss: 2.3172667517381558\n",
            "Epoch 31/50, Loss: 2.317672648393285\n",
            "Epoch 32/50, Loss: 2.3169905313140595\n",
            "Epoch 33/50, Loss: 2.309438640504237\n",
            "Epoch 34/50, Loss: 2.312911414429355\n",
            "Epoch 35/50, Loss: 2.3099841053223673\n",
            "Epoch 36/50, Loss: 2.3057820827454862\n",
            "Epoch 37/50, Loss: 2.301588676164827\n",
            "Epoch 38/50, Loss: 2.3049857698742997\n",
            "Epoch 39/50, Loss: 2.307801471037023\n",
            "Epoch 40/50, Loss: 2.299579174012479\n",
            "Epoch 41/50, Loss: 2.305580024524113\n",
            "Epoch 42/50, Loss: 2.2974166098762963\n",
            "Epoch 43/50, Loss: 2.3000682382022632\n",
            "Epoch 44/50, Loss: 2.2965724416401074\n",
            "Epoch 45/50, Loss: 2.295570924154023\n",
            "Epoch 46/50, Loss: 2.298661177115672\n",
            "Epoch 47/50, Loss: 2.294053918565326\n",
            "Epoch 48/50, Loss: 2.29115950390506\n",
            "Epoch 49/50, Loss: 2.288895210341724\n",
            "Epoch 50/50, Loss: 2.294755395720987\n",
            "Accuracy: 33.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1 regularization with spatial loss\n"
      ],
      "metadata": {
        "id": "uvtw8XdzYg4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the fully connected network with batch normalization and dropout\n",
        "class FullyConnectedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32, 1024)\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.fc4 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32)  # Flatten the input\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = FullyConnectedNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to apply L1 regularization\n",
        "def l1_regularization(model, lambda_l1):\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "    return lambda_l1 * l1_norm\n",
        "\n",
        "# Customized spatial loss function\n",
        "def spatial_loss(outputs):\n",
        "    num_neurons = outputs.size(1)\n",
        "\n",
        "    # Calculate pairwise correlations\n",
        "    outputs_centered = outputs - outputs.mean(dim=0)\n",
        "    pairwise_correlations = torch.matmul(outputs_centered.T, outputs_centered) / (outputs_centered.size(0) - 1)\n",
        "    pairwise_correlations /= (outputs_centered.std(dim=0).unsqueeze(1) * outputs_centered.std(dim=0).unsqueeze(0) + 1e-8)\n",
        "\n",
        "    # Calculate the grid size and ensure it is a perfect square\n",
        "    grid_size = int(torch.sqrt(torch.tensor(num_neurons, dtype=torch.float32)))\n",
        "    if grid_size * grid_size != num_neurons:\n",
        "        raise ValueError(\"Number of neurons must be a perfect square to form a grid.\")\n",
        "\n",
        "    # Calculate distances in the grid\n",
        "    distances = torch.zeros((num_neurons, num_neurons))\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            for k in range(grid_size):\n",
        "                for l in range(grid_size):\n",
        "                    index_1 = i * grid_size + j\n",
        "                    index_2 = k * grid_size + l\n",
        "                    distance = torch.sqrt(torch.tensor((i - k)**2 + (j - l)**2, dtype=torch.float32))\n",
        "                    distances[index_1, index_2] = 1 / (distance + 1)\n",
        "\n",
        "    # Compute the spatial loss\n",
        "    SL = torch.mean(torch.abs(pairwise_correlations - distances))\n",
        "\n",
        "    return SL\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 50\n",
        "lambda_l1 = 0.0001\n",
        "lambda_spatial = 0.001  # Weight for the spatial loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss += l1_regularization(model, lambda_l1)\n",
        "\n",
        "        # Calculate spatial loss for the outputs of the second last layer\n",
        "        second_last_layer_output = model.dropout(model.bn3(model.fc3(torch.relu(model.bn2(model.fc2(torch.relu(model.bn1(model.fc1(data.view(-1, 32 * 32))))))))))\n",
        "        sl = spatial_loss(second_last_layer_output)\n",
        "        loss += lambda_spatial * sl\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "print(f'Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "McyChaLkYmtx",
        "outputId": "591791b4-c376-4f8d-cd86-b5363f81b22b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/50, Loss: 3.7664496349861554\n",
            "Epoch 2/50, Loss: 3.1731145360585673\n",
            "Epoch 3/50, Loss: 3.089784645363498\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3366896d5b05>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Calculate spatial loss for the outputs of the second last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0msecond_last_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_last_layer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlambda_spatial\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3366896d5b05>\u001b[0m in \u001b[0;36mspatial_loss\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mindex_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Compute the spatial loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}